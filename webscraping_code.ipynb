{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save articles as txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook scrapes the text of news articles off the web (URLs are contained in a csv file) and saves them as txt files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as no\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/albertomicheletti/Documents/Hackaton'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/albertomicheletti/Documents/Hackaton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_txt_path = os.getcwd() + \"/articles_txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv(os.getcwd() + \"/Heatwaves-V1_pandas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOT THE RIGHT SOLUTION\n",
    "# # Add a column with title.txt\n",
    "# articles['Title txt'] = 'Article ' + articles['Title'].astype(str) + '.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an article ID column\n",
    "articles['Article_ID'] = articles.index\n",
    "\n",
    "# Change dtype to object  \n",
    "articles['Article_ID'] = articles['Article_ID'].astype('object')   \n",
    "\n",
    "# Convert values to strings\n",
    "articles['Article_ID'] = articles['Article_ID'].apply(str) \n",
    "\n",
    "# Concatenate with string\n",
    "articles['Article_ID'] = 'ART-' + articles['Article_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only UK articles\n",
    "articles_UK = articles.loc[articles[\"Country\"] == \"United Kingdom\",:].reset_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More than 2 mil visitors\n",
    "articles_UK_2mil = articles_UK.loc[articles_UK[\"Total Monthly Visitors\"] > 2000000, :].reset_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3585, 14)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_UK_2mil.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add political leaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['telegraph.co.uk', 'mirror.co.uk', 'bbc.co.uk', 'yahoo.com',\n",
       "       'player.fm', 'timeout.com', 'itv.com', 'theguardian.com',\n",
       "       'aol.co.uk', 'bbc.com', 'dailymail.co.uk', 'express.co.uk',\n",
       "       'liverpoolecho.co.uk', 'investing.com', 'huffingtonpost.co.uk',\n",
       "       'metro.co.uk', 'dailystar.co.uk', 'sky.com',\n",
       "       'manchestereveningnews.co.uk', 'lse.ac.uk', 'standard.co.uk',\n",
       "       'independent.co.uk', 'thetimes.co.uk', 'ladbible.com',\n",
       "       'euronews.com', 'tportal.hr', 'nature.com', 'phys.org',\n",
       "       'youtube.com', 'wordpress.com', 'hurriyet.com.tr', 'cnbc.com',\n",
       "       'pistonheads.com', 'mumsnet.com', 'nottingham.ac.uk',\n",
       "       'theconversation.com', 'thesun.co.uk', 'thestudentroom.co.uk',\n",
       "       'channel4.com', 'bmj.com', 'www.gov.uk', 'theweathernetwork.com',\n",
       "       'worldcat.org', 'indy100.com', 'iflscience.com', 'marca.com',\n",
       "       'cnn.com', 'zerohedge.com', 'autosport.com', 'radiotimes.com',\n",
       "       'wpengine.com', 'halifax.co.uk', 'boredpanda.com'], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_UK_2mil[\"Domain\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fed this into Claude\n",
    "# The prompt was \"create a pandas dataframe with the news sources below as one column and their political leaning as the other column\"\n",
    "# then I asked: Can you write it as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dict = {\n",
    "  'telegraph.co.uk': 'Right',\n",
    "  'mirror.co.uk': 'Left',\n",
    "  'bbc.co.uk': 'Centre',\n",
    "  'yahoo.com': 'No leaning',\n",
    "  'player.fm': 'No leaning',\n",
    "  'timeout.com': 'No leaning',\n",
    "  'itv.com': 'Centre',\n",
    "  'theguardian.com': 'Left/Centre-left',\n",
    "  'aol.co.uk': 'No leaning',\n",
    "  'bbc.com': 'Centre',\n",
    "  'dailymail.co.uk': 'Right',\n",
    "  'express.co.uk': 'Right',\n",
    "  'liverpoolecho.co.uk': 'Centre',\n",
    "  'investing.com': 'No leaning',\n",
    "  'huffingtonpost.co.uk': 'Left/Centre-left',\n",
    "  'metro.co.uk': 'Centre-left',\n",
    "  'dailystar.co.uk': 'Right',\n",
    "  'sky.com': 'Centre',\n",
    "  'manchestereveningnews.co.uk': 'Centre',\n",
    "  'lse.ac.uk': 'No leaning',\n",
    "  'standard.co.uk': 'Centre-right',\n",
    "  'independent.co.uk': 'Centre-left',\n",
    "  'thetimes.co.uk': 'Centre-right',\n",
    "  'ladbible.com': 'No leaning',\n",
    "  'euronews.com': 'No leaning',\n",
    "  'tportal.hr': 'No leaning',\n",
    "  'nature.com': 'No leaning',\n",
    "  'phys.org': 'No leaning',\n",
    "  'youtube.com': 'No leaning',\n",
    "  'wordpress.com': 'No leaning',\n",
    "  'hurriyet.com.tr': 'No leaning',\n",
    "  'cnbc.com': 'Centre',\n",
    "  'pistonheads.com': 'No leaning',\n",
    "  'mumsnet.com': 'No leaning',\n",
    "  'nottingham.ac.uk': 'No leaning',\n",
    "  'theconversation.com': 'Centre-left',\n",
    "  'thesun.co.uk': 'Right',\n",
    "  'thestudentroom.co.uk': 'No leaning',\n",
    "  'channel4.com': 'No leaning',\n",
    "  'bmj.com': 'No leaning',\n",
    "  'www.gov.uk': 'No leaning',\n",
    "  'theweathernetwork.com': 'No leaning',\n",
    "  'worldcat.org': 'No leaning',\n",
    "  'indy100.com': 'No leaning',\n",
    "  'iflscience.com': 'No leaning',\n",
    "  'marca.com': 'No leaning',\n",
    "  'cnn.com': 'Centre',\n",
    "  'zerohedge.com': 'No leaning',\n",
    "  'autosport.com': 'No leaning',\n",
    "  'radiotimes.com': 'No leaning',\n",
    "  'wpengine.com': 'No leaning',\n",
    "  'halifax.co.uk': 'No leaning',\n",
    "  'boredpanda.com': 'No leaning'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leaning(domain):\n",
    "  return news_dict.get(domain, 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_UK_2mil['Leaning'] = articles_UK_2mil['Domain'].apply(get_leaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Right', 'Left', 'Centre', 'No leaning', 'Left/Centre-left',\n",
       "       'Centre-left', 'Centre-right'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_UK_2mil['Leaning'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_leaning(leaning):\n",
    "  if leaning == 'Centre':\n",
    "    return 'Centre'\n",
    "  elif leaning in ['Right','Centre-right']: \n",
    "    return 'Right'\n",
    "  elif leaning in ['Left', 'Left/Centre-left', \"Centre-left\"]:\n",
    "    return 'Left'\n",
    "  else:\n",
    "    return leaning\n",
    "  \n",
    "articles_UK_2mil['Leaning_simple'] = articles_UK_2mil['Leaning'].apply(simplify_leaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Right', 'Left', 'Centre', 'No leaning'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Four category leaning\n",
    "articles_UK_2mil['Leaning_simple'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced sample VERSION 1\n",
    "# Get unique leanings  \n",
    "leanings = articles_UK_2mil['Leaning_simple'].unique()\n",
    "\n",
    "# Sample 10 articles per leaning\n",
    "balanced_sample = pd.DataFrame()\n",
    "for l in leanings:\n",
    "    temp = articles_UK_2mil[articles_UK_2mil['Leaning_simple'] == l].sample(n=10, random_state=1)\n",
    "    balanced_sample = pd.concat([balanced_sample, temp])\n",
    "\n",
    "# Shuffle the rows    \n",
    "balanced_sample = balanced_sample.sample(frac=1)#.reset_index(drop=True)\n",
    "\n",
    "# Take first 10 rows\n",
    "balanced_sample = balanced_sample.iloc[:10]\n",
    "\n",
    "# Create a list\n",
    "balanced_list = balanced_sample.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced sample VERSION 2\n",
    "# Get unique leanings  \n",
    "leanings = articles_UK_2mil['Leaning_simple'].unique()\n",
    "\n",
    "# Sample 10 articles per leaning\n",
    "balanced_sample = pd.DataFrame()\n",
    "for l in leanings:\n",
    "    temp = articles_UK_2mil[articles_UK_2mil['Leaning_simple'] == l].sample(n=2, random_state=12)\n",
    "    balanced_sample = pd.concat([balanced_sample, temp])\n",
    "\n",
    "# # Shuffle the rows    \n",
    "# balanced_sample = balanced_sample.sample(frac=1)#.reset_index(drop=True)\n",
    "\n",
    "# # Take first 10 rows\n",
    "# balanced_sample = balanced_sample.iloc[:10]\n",
    "\n",
    "# Create a list\n",
    "balanced_list = balanced_sample.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[971, 1556, 1700, 3020, 2348, 1151, 1596, 677]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(balanced_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>Query Id</th>\n",
       "      <th>Query Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Url</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Language</th>\n",
       "      <th>Country</th>\n",
       "      <th>Full Text</th>\n",
       "      <th>Total Monthly Visitors</th>\n",
       "      <th>Article_ID</th>\n",
       "      <th>Leaning</th>\n",
       "      <th>Leaning_simple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>5154</td>\n",
       "      <td>14149</td>\n",
       "      <td>2001017906</td>\n",
       "      <td>Heatwaves</td>\n",
       "      <td>02:33.0</td>\n",
       "      <td>BBC climate editor accused of hypocrisy as he ...</td>\n",
       "      <td>http://www.telegraph.co.uk/news/2023/07/19/jus...</td>\n",
       "      <td>telegraph.co.uk</td>\n",
       "      <td>negative</td>\n",
       "      <td>en</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>By Patrick Sawer, Senior News Reporter 19 July...</td>\n",
       "      <td>4.457000e+07</td>\n",
       "      <td>ART-14149</td>\n",
       "      <td>Right</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>6838</td>\n",
       "      <td>20105</td>\n",
       "      <td>2001017906</td>\n",
       "      <td>Heatwaves</td>\n",
       "      <td>06:00.0</td>\n",
       "      <td>List of countries impacted by Europe heatwave ...</td>\n",
       "      <td>http://ct.moreover.com/?a=51326477524&amp;p=2p3&amp;v=...</td>\n",
       "      <td>dailystar.co.uk</td>\n",
       "      <td>neutral</td>\n",
       "      <td>en</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Due to licensing restrictions, this mention ca...</td>\n",
       "      <td>1.150000e+07</td>\n",
       "      <td>ART-20105</td>\n",
       "      <td>Right</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>7194</td>\n",
       "      <td>21541</td>\n",
       "      <td>2001017906</td>\n",
       "      <td>Heatwaves</td>\n",
       "      <td>00:00.0</td>\n",
       "      <td>Met Office gives update on whether UK will hit...</td>\n",
       "      <td>http://ct.moreover.com/?a=51326058462&amp;p=2p3&amp;v=...</td>\n",
       "      <td>independent.co.uk</td>\n",
       "      <td>neutral</td>\n",
       "      <td>en</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Due to licensing restrictions, this mention ca...</td>\n",
       "      <td>4.116800e+07</td>\n",
       "      <td>ART-21541</td>\n",
       "      <td>Centre-left</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3020</th>\n",
       "      <td>11751</td>\n",
       "      <td>36699</td>\n",
       "      <td>2001017906</td>\n",
       "      <td>Heatwaves</td>\n",
       "      <td>00:00.0</td>\n",
       "      <td>El Nino could intensify record-breaking heat a...</td>\n",
       "      <td>http://ct.moreover.com/?a=51295880008&amp;p=2p3&amp;v=...</td>\n",
       "      <td>independent.co.uk</td>\n",
       "      <td>neutral</td>\n",
       "      <td>en</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Due to licensing restrictions, this mention ca...</td>\n",
       "      <td>4.116800e+07</td>\n",
       "      <td>ART-36699</td>\n",
       "      <td>Centre-left</td>\n",
       "      <td>Left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>9356</td>\n",
       "      <td>28695</td>\n",
       "      <td>2001017906</td>\n",
       "      <td>Heatwaves</td>\n",
       "      <td>00:00.0</td>\n",
       "      <td>BBCNEEU, Jul 16, 2023 01:00 PM GMT - BBC News</td>\n",
       "      <td>http://ct.moreover.com/?a=51311871996&amp;p=2p3&amp;v=...</td>\n",
       "      <td>bbc.com</td>\n",
       "      <td>positive</td>\n",
       "      <td>en</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Due to licensing restrictions, this mention ca...</td>\n",
       "      <td>2.059700e+08</td>\n",
       "      <td>ART-28695</td>\n",
       "      <td>Centre</td>\n",
       "      <td>Centre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>5734</td>\n",
       "      <td>16397</td>\n",
       "      <td>2001017906</td>\n",
       "      <td>Heatwaves</td>\n",
       "      <td>01:00.0</td>\n",
       "      <td>BBC1SE, Jul 19, 2023 04:01 AM BST - BBC News</td>\n",
       "      <td>http://ct.moreover.com/?a=51332039902&amp;p=2p3&amp;v=...</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>neutral</td>\n",
       "      <td>en</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Due to licensing restrictions, this mention ca...</td>\n",
       "      <td>5.300000e+08</td>\n",
       "      <td>ART-16397</td>\n",
       "      <td>Centre</td>\n",
       "      <td>Centre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6944</td>\n",
       "      <td>20603</td>\n",
       "      <td>2001017906</td>\n",
       "      <td>Heatwaves</td>\n",
       "      <td>27:16.0</td>\n",
       "      <td>Brits travelling to Europe given updated trave...</td>\n",
       "      <td>https://uk.sports.yahoo.com/news/brits-travell...</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>negative</td>\n",
       "      <td>en</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>The UK Foreign Office has updated its travel a...</td>\n",
       "      <td>7.700000e+10</td>\n",
       "      <td>ART-20603</td>\n",
       "      <td>No leaning</td>\n",
       "      <td>No leaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>4022</td>\n",
       "      <td>9926</td>\n",
       "      <td>2001017906</td>\n",
       "      <td>Heatwaves</td>\n",
       "      <td>44:55.0</td>\n",
       "      <td>Watch as wildfires continue to rage in Greece ...</td>\n",
       "      <td>http://ct.moreover.com/?a=51352047384&amp;p=2p3&amp;v=...</td>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>neutral</td>\n",
       "      <td>en</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Due to licensing restrictions, this mention ca...</td>\n",
       "      <td>7.700000e+10</td>\n",
       "      <td>ART-9926</td>\n",
       "      <td>No leaning</td>\n",
       "      <td>No leaning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      level_0  index    Query Id Query Name     Date  \\\n",
       "971      5154  14149  2001017906  Heatwaves  02:33.0   \n",
       "1556     6838  20105  2001017906  Heatwaves  06:00.0   \n",
       "1700     7194  21541  2001017906  Heatwaves  00:00.0   \n",
       "3020    11751  36699  2001017906  Heatwaves  00:00.0   \n",
       "2348     9356  28695  2001017906  Heatwaves  00:00.0   \n",
       "1151     5734  16397  2001017906  Heatwaves  01:00.0   \n",
       "1596     6944  20603  2001017906  Heatwaves  27:16.0   \n",
       "677      4022   9926  2001017906  Heatwaves  44:55.0   \n",
       "\n",
       "                                                  Title  \\\n",
       "971   BBC climate editor accused of hypocrisy as he ...   \n",
       "1556  List of countries impacted by Europe heatwave ...   \n",
       "1700  Met Office gives update on whether UK will hit...   \n",
       "3020  El Nino could intensify record-breaking heat a...   \n",
       "2348      BBCNEEU, Jul 16, 2023 01:00 PM GMT - BBC News   \n",
       "1151       BBC1SE, Jul 19, 2023 04:01 AM BST - BBC News   \n",
       "1596  Brits travelling to Europe given updated trave...   \n",
       "677   Watch as wildfires continue to rage in Greece ...   \n",
       "\n",
       "                                                    Url             Domain  \\\n",
       "971   http://www.telegraph.co.uk/news/2023/07/19/jus...    telegraph.co.uk   \n",
       "1556  http://ct.moreover.com/?a=51326477524&p=2p3&v=...    dailystar.co.uk   \n",
       "1700  http://ct.moreover.com/?a=51326058462&p=2p3&v=...  independent.co.uk   \n",
       "3020  http://ct.moreover.com/?a=51295880008&p=2p3&v=...  independent.co.uk   \n",
       "2348  http://ct.moreover.com/?a=51311871996&p=2p3&v=...            bbc.com   \n",
       "1151  http://ct.moreover.com/?a=51332039902&p=2p3&v=...          bbc.co.uk   \n",
       "1596  https://uk.sports.yahoo.com/news/brits-travell...          yahoo.com   \n",
       "677   http://ct.moreover.com/?a=51352047384&p=2p3&v=...          yahoo.com   \n",
       "\n",
       "     Sentiment Language         Country  \\\n",
       "971   negative       en  United Kingdom   \n",
       "1556   neutral       en  United Kingdom   \n",
       "1700   neutral       en  United Kingdom   \n",
       "3020   neutral       en  United Kingdom   \n",
       "2348  positive       en  United Kingdom   \n",
       "1151   neutral       en  United Kingdom   \n",
       "1596  negative       en  United Kingdom   \n",
       "677    neutral       en  United Kingdom   \n",
       "\n",
       "                                              Full Text  \\\n",
       "971   By Patrick Sawer, Senior News Reporter 19 July...   \n",
       "1556  Due to licensing restrictions, this mention ca...   \n",
       "1700  Due to licensing restrictions, this mention ca...   \n",
       "3020  Due to licensing restrictions, this mention ca...   \n",
       "2348  Due to licensing restrictions, this mention ca...   \n",
       "1151  Due to licensing restrictions, this mention ca...   \n",
       "1596  The UK Foreign Office has updated its travel a...   \n",
       "677   Due to licensing restrictions, this mention ca...   \n",
       "\n",
       "      Total Monthly Visitors Article_ID      Leaning Leaning_simple  \n",
       "971             4.457000e+07  ART-14149        Right          Right  \n",
       "1556            1.150000e+07  ART-20105        Right          Right  \n",
       "1700            4.116800e+07  ART-21541  Centre-left           Left  \n",
       "3020            4.116800e+07  ART-36699  Centre-left           Left  \n",
       "2348            2.059700e+08  ART-28695       Centre         Centre  \n",
       "1151            5.300000e+08  ART-16397       Centre         Centre  \n",
       "1596            7.700000e+10  ART-20603   No leaning     No leaning  \n",
       "677             7.700000e+10   ART-9926   No leaning     No leaning  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs written to urls.txt\n"
     ]
    }
   ],
   "source": [
    "# Extract the Url column  \n",
    "urls = balanced_sample['Url']\n",
    "\n",
    "# Write the URLs to a text file  \n",
    "with open('urls.txt', 'w') as f:\n",
    "    for url in urls:\n",
    "        f.write(url + '\\n')\n",
    "        \n",
    "print('URLs written to urls.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subarticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "subarticles = articles_UK_2mil.iloc[balanced_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from datetime import datetime\n",
    "\n",
    "# def save_article_to_txt(url, filename):\n",
    "#     try:\n",
    "#         # Set the timeout to 60 seconds (1 minute)\n",
    "#         response = requests.get(url, timeout=60)\n",
    "#         response.raise_for_status()\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         article_text = ''\n",
    "#         for paragraph in soup.find_all('p'):\n",
    "#             article_text += paragraph.get_text() + '\\n'\n",
    "        \n",
    "#         folder_name = os.getcwd() + \"/articles_txt\"\n",
    "#         os.makedirs(folder_name, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "#         file_path = os.path.join(folder_name, filename + \".txt\")\n",
    "        \n",
    "#         with open(file_path, 'w', encoding='utf-8') as file:\n",
    "#             file.write(article_text)\n",
    "#         return 1  # Success flag: 1 indicates successful retrieval and save\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Error fetching the webpage: {e}\")\n",
    "#         return 0  # Success flag: 0 indicates unsuccessful retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scrape multiple articles\n",
    "# subarticles['Success'] = subarticles.apply(lambda row: save_article_to_txt(row['Url'], row['Article_ID']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def save_article_to_txt(url, filename, title, domain):\n",
    "    try:\n",
    "        # Set the timeout to 60 seconds (1 minute)\n",
    "        response = requests.get(url, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_text = ''\n",
    "        for paragraph in soup.find_all('p'):\n",
    "            article_text += paragraph.get_text() + '\\n'\n",
    "        \n",
    "        # Combine title, domain, and article text\n",
    "        article_content = f\"Title: {title}\\nDomain: {domain}\\n\\n{article_text}\"\n",
    "\n",
    "        folder_name = os.getcwd() + \"/articles_txt\"\n",
    "        os.makedirs(folder_name, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "        file_path = os.path.join(folder_name, filename + \".txt\")\n",
    "        \n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(article_content)\n",
    "        return 1  # Success flag: 1 indicates successful retrieval and save\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the webpage: {e}\")\n",
    "        return 0  # Success flag: 0 indicates unsuccessful retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w3/yvxk0yvs58q_7b2jtnry065m0000gn/T/ipykernel_4508/3536236092.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subarticles['Success'] = subarticles.apply(lambda row: save_article_to_txt(row['Url'], row['Article_ID'], row['Title'], row['Domain']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Scrape multiple articles\n",
    "subarticles['Success'] = subarticles.apply(lambda row: save_article_to_txt(row['Url'], row['Article_ID'], row['Title'], row['Domain']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "971     1\n",
       "1556    1\n",
       "1700    1\n",
       "3020    1\n",
       "2348    1\n",
       "1151    1\n",
       "1596    1\n",
       "677     1\n",
       "Name: Success, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subarticles[\"Success\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files concatenated successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "folder = articles_txt_path\n",
    "\n",
    "# Get list of .txt files in folder\n",
    "files = [f for f in os.listdir(folder) if f.endswith('.txt')] \n",
    "\n",
    "with open('combined.txt', 'w') as outfile:\n",
    "    for fname in files:\n",
    "        filepath = path.join(folder, fname)\n",
    "        with open(filepath) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "        outfile.write(\"\\n\") # blank line between files\n",
    "        \n",
    "print(\"Files concatenated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.dailymail.co.uk/wires/reuters/article-12323727/WHO-warns-dengue-risk-global-warming-pushes-cases-near-historic-highs.html'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.loc[8012, \"Url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
